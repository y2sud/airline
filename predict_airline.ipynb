{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, svm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import os, sys, time\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define models\n",
    "ridgecv = linear_model.RidgeCV(alphas=[0.1,1,10])\n",
    "lasso = linear_model.Lasso(alpha=0.1)\n",
    "ridge = linear_model.Ridge(alpha=0.1, fit_intercept=False)\n",
    "svr = svm.SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/test/prsnl/airline'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'/home/test/prsnl/airline'\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    global data2, plane, airports, carriers\n",
    "    \n",
    "    data_2007 = pd.read_csv('2007.csv')    \n",
    "    print len(data_2007)\n",
    "    data_2008 = pd.read_csv('2008.csv')\n",
    "    print len(data_2008)\n",
    "    data2 = pd.concat([data_2007, data_2008],axis=0)\n",
    "    data2.reset_index(inplace=True)\n",
    "    plane = pd.read_csv('plane-data.csv')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7453215\n",
      "7009728\n"
     ]
    }
   ],
   "source": [
    "read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just take first 1 million rows from 2007 sheet\n",
    "data = data2.ix[:10000000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sys.getsizeof(data) / 1000000\n",
    "del data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_all = data.ix[:,'ArrDelay'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-6557e36d364a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# These are categorical columns. Transform them to dummy variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# a df named out contains all these dummy variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-61-6557e36d364a>\u001b[0m in \u001b[0;36mcreate_dummies\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print cat.info()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/test/miniconda2/lib/python2.7/site-packages/pandas/core/reshape.pyc\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         result = _get_dummies_1d(data, prefix, prefix_sep, dummy_na,\n\u001b[0;32m-> 1104\u001b[0;31m                                  sparse=sparse, drop_first=drop_first)\n\u001b[0m\u001b[1;32m   1105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/test/miniconda2/lib/python2.7/site-packages/pandas/core/reshape.pyc\u001b[0m in \u001b[0;36m_get_dummies_1d\u001b[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0mdummy_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdummy_na\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Select below few columns\n",
    "\n",
    "lst = ['Month', 'DayofMonth', 'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest']\n",
    "out = pd.DataFrame()\n",
    "type(out)\n",
    "\n",
    "def create_dummies():\n",
    "    out = pd.DataFrame()\n",
    "    for col in lst:\n",
    "        cat = pd.get_dummies(data.ix[:,col])\n",
    "        #print cat.info()\n",
    "        out = pd.concat([out,cat], axis=1)\n",
    "    return out\n",
    "        \n",
    "# These are categorical columns. Transform them to dummy variables\n",
    "# a df named out contains all these dummy variables\n",
    "out = create_dummies()\n",
    "out.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create df named out2 for elapsed time & distance, which are already in numeric format\n",
    "out2 = data.ix[:,['CRSElapsedTime','Distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Subset expected departure time into six 4-hour buckets\n",
    "deptime = data.ix[:,'CRSDepTime']\n",
    "per_series = []\n",
    "for ind, row in deptime.iteritems():\n",
    "    #print row\n",
    "    if row < 400:\n",
    "        period = 1\n",
    "    elif row < 800:\n",
    "        period = 2\n",
    "    elif row < 1200:\n",
    "        period = 3\n",
    "    elif row < 1600:\n",
    "        period = 4\n",
    "    elif row < 2000:\n",
    "        period = 5\n",
    "    else:\n",
    "        period = 6\n",
    "    \n",
    "    per_series.append(period)\n",
    "    \n",
    "\n",
    "# out3 will contain departure time dummy variables\n",
    "out3 = pd.get_dummies(pd.Series(per_series))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uniq_tail = data.ix[:,'TailNum'].drop_duplicates().dropna()\n",
    "#uniq_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will identify unique tail numbers from 2007/2008 data; get year from plane-data, and build a list of years \n",
    "# for corresponding tail numbers\n",
    "uniq_tail = data.ix[:,'TailNum'].drop_duplicates().dropna()\n",
    "uniq_tail.reset_index()\n",
    "print len(uniq_tail)\n",
    "year_list = []\n",
    "for ind, tail in uniq_tail.iteritems():\n",
    "    #print 'ind ', ind\n",
    "    year = plane.ix[plane.ix[:,'tailnum'] == tail,'year']\n",
    "    year.reset_index(inplace=True,drop=True)\n",
    "    #year = pd.to_numeric(year_ser)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        if year.empty:\n",
    "            #print 'year1', year\n",
    "            decade = '00-00'\n",
    "            year_list.append(decade)\n",
    "            continue\n",
    "    except:\n",
    "        print 'year2 ', year\n",
    "        \n",
    "    \n",
    "\n",
    "    if year.item() < 1960:\n",
    "        decade = '50-60'\n",
    "    elif year.item() < 1970:\n",
    "        decade = '60-70'\n",
    "    elif year.item() < 1980:\n",
    "        decade = '70-80'\n",
    "    elif year.item() < 1990:\n",
    "        decade = '80-90'\n",
    "    elif year.item() < 2000:\n",
    "        decade = '90-00'\n",
    "    else:\n",
    "        decade = '00-10'\n",
    "\n",
    "        \n",
    "    year_list.append(decade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine tail number with corresponding year into a dataframe\n",
    "year_series = pd.Series(year_list)\n",
    "year_series.reset_index(inplace=True,drop=True)\n",
    "uniq_tail.reset_index(inplace=True,drop=True)\n",
    "print len(year_series)\n",
    "tail_year = pd.concat([uniq_tail,year_series],axis=1)\n",
    "#print year_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0.1, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory error... reduce the count of rows\n",
    "# Y is target\n",
    "limit = 800000\n",
    "Y = Y_all[:limit+1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find whether there are any NaNs in target variable - Arrival delay\n",
    "# There were few such rows...\n",
    "Y_np = np.array(Y)\n",
    "print 'Are any of the values Null for Y? ' ,  np.any(np.isnan(np.array(Y)))\n",
    "print 'Are values finite for Y? ', np.any(np.isfinite(np.array(Y)))\n",
    "print 'count of nulls in 3 diff approaches ', Y.isnull().sum().sum() , np.sum(np.isnan(Y_np)), \\\n",
    "        pd.isnull(Y_np).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find index of rows that contain NaN in target variable\n",
    "null_inds = np.where(np.isnan(Y_np))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print null_inds, len(null_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove rows Y where NaN was present in target variable\n",
    "Y.drop(Y.index[null_inds], inplace=True)\n",
    "Y_np = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where actual modeling happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling\n",
    "\n",
    "# Concatenate all the feature data frames\n",
    "new_temp_X = data.ix[:limit,['CarrierDelay','WeatherDelay', 'NASDelay', 'SecurityDelay']]\n",
    "temp_X = pd.concat([new_temp_X, out3.ix[:limit,], out2.ix[:limit,], out.ix[:limit,]],axis=1)\n",
    "\n",
    "print temp_X.shape\n",
    "print 'Count of nulls ' , np.isnan(temp_X).sum().sum()\n",
    "\n",
    "temp_X.drop(temp_X.index[null_inds], inplace=True)\n",
    "print 'Count of nulls after processing..' , np.isnan(temp_X).sum().sum()\n",
    "print temp_X.shape\n",
    "#len(temp_X[temp_X[:] == 0.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print all column names\n",
    "#temp_X.columns.values\n",
    "#temp_X['CRSElapsedTime'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = ridge\n",
    "#clf = lasso\n",
    "#clf = svr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration:  49.137706995\n",
      "Score; average score  [ 0.46801901  0.58914478  0.5708272   0.67176553  0.71454111] 0.602859525262\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "temp_X_np = np.array(temp_X)\n",
    "score = cross_val_score(clf, temp_X_np, Y_np, cv=5)\n",
    "print 'duration: ', time.time() - st\n",
    "print 'Score; average score ' , score, np.average(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = lasso.fit(temp_X_np, Y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.64735818671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0218866235281814"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find linear regression coefficient. Second value is for weather delay\n",
    "print pred.intercept_\n",
    "pred.coef_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find pvalue for weather delay\n",
    "scores, pvalues = chi2(temp_X_np, Y_np)\n",
    "\n",
    "#pvalues[np.where(pvalues[:] > 0.05)]\n",
    "pvalues[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Pearson Correlation between weather & arrival delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_vect = data.ix[:,'WeatherDelay'].copy()\n",
    "arr_vect = data.ix[:,'ArrDelay'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000001,), (10000001,))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_vect.shape, arr_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nulls in weather & arrival delay vectors?  True False\n",
      "0 1859563\n",
      "nulls in weather & arrival delay vectors?  False False\n"
     ]
    }
   ],
   "source": [
    "print 'nulls in weather & arrival delay vectors? ' , weather_vect.isnull().any(), arr_vect.isnull().any()\n",
    "null_inds = np.where(arr_vect.isnull())\n",
    "null2_inds = np.where(weather_vect.isnull())\n",
    "print len(null_inds[0]), len(null2_inds[0])\n",
    "\n",
    "# drop rows that have null in arrival delay vector \n",
    "weather_vect.drop(weather_vect.index[null_inds], inplace=True)\n",
    "arr_vect.drop(arr_vect.index[null_inds], inplace=True)\n",
    "weather_vect.drop(weather_vect.index[null2_inds], inplace=True)\n",
    "arr_vect.drop(arr_vect.index[null2_inds], inplace=True)\n",
    "print 'nulls in weather & arrival delay vectors? ' , weather_vect.isnull().any(), arr_vect.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.28242394418128486, 0.0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(weather_vect, arr_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
